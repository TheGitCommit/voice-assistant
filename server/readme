# Voice Assistant Server

Local voice assistant server running Whisper STT, Llama LLM, and Piper TTS on a laptop with GPU acceleration.

## Features

- üé§ Real-time speech-to-text (Faster Whisper)
- ü§ñ Local LLM inference (Llama.cpp)
- üîä Text-to-speech synthesis (Piper)
- üåê WebSocket streaming for low latency
- ‚ö° GPU-accelerated (CUDA)
- üéØ Energy-based VAD for utterance segmentation

## Requirements

- **OS**: Windows (WSL/Linux with minor path adjustments)
- **GPU**: NVIDIA GPU with CUDA support
- **RAM**: 16GB+ recommended
- **Python**: 3.10+

### External Dependencies

1. **Llama.cpp server** ([llama.cpp](https://github.com/ggerganov/llama.cpp))
   - Build with CUDA support
   - Download a GGUF model (e.g., Llama-3.1-8B-Instruct)

2. **Piper TTS** ([Piper](https://github.com/rhasspy/piper))
   - Download `piper.exe` (Windows) or compile for Linux
   - Download a voice model (e.g., `en_US-amy-medium.onnx`)

## Installation

```bash
# Clone repository
git clone <repo-url>
cd voice-assistant/server

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure paths in server/config.py
# Edit the paths to match your llama.cpp and Piper installations
```

## Configuration

Edit `server/config.py`:

```python
CONFIG = {
    "llama": LlamaConfig(
        exe_path=r"C:\path\to\llama-server.exe",
        model_path=r"C:\path\to\model.gguf",
    ),
    "piper": PiperConfig(
        exe_path=r"C:\path\to\piper.exe",
        model_path=r"C:\path\to\voice-model.onnx",
    ),
    # ... other configs
}
```

## Usage

```bash
# Activate virtual environment
source venv/bin/activate  # Windows: venv\Scripts\activate

# Run server
python -m server.main

# Server starts on http://localhost:8000
# WebSocket endpoint: ws://localhost:8000/ws/audio
# Health check: http://localhost:8000/health
```

The server will:
1. Start Llama.cpp subprocess
2. Wait ~10s for model loading
3. Accept WebSocket connections from clients

## Architecture

```
Client Audio ‚Üí WebSocket ‚Üí VAD ‚Üí Whisper STT ‚Üí Llama LLM ‚Üí Piper TTS ‚Üí Client
```

### Key Components

- **VAD** (`vad.py`): Energy-based speech boundary detection
- **AudioProcessor** (`audio_processor.py`): Pipeline orchestration
- **WebSocketConnection** (`websocket_connection.py`): Connection management
- **LLMClient** (`llm_client.py`): Llama.cpp HTTP client
- **WhisperSTT** (`whisper_stt.py`): Speech transcription
- **PiperTTS** (`piper_tts.py`): Speech synthesis

## API

### WebSocket Protocol

**Client ‚Üí Server:**
- Binary: Raw PCM audio (float32, 16kHz, mono, 20ms chunks)
- JSON: `{"type": "hello", "sample_rate": 16000, "channels": 1}`

**Server ‚Üí Client:**
- Binary: TTS audio (PCM16LE, 22050Hz, mono)
- JSON: `{"type": "transcription", "text": "..."}`
- JSON: `{"type": "llm_response", "text": "..."}`

## Logging

Logs include:
- Connection lifecycle
- Audio processing status
- VAD state transitions
- Transcription and LLM responses
- Performance metrics

## Troubleshooting

**"Llama server failed to start"**
- Check paths in `config.py`
- Verify llama-server.exe and model exist
- Check CUDA is available

**"Piper executable not found"**
- Verify Piper paths in `config.py`
- Ensure `.onnx` and `.onnx.json` files exist

**"CUDA out of memory"**
- Reduce `gpu_layers` in LlamaConfig
- Use smaller models
- Close other GPU applications

**Poor transcription quality**
- Check microphone audio quality
- Adjust VAD thresholds in config
- Ensure 16kHz sample rate

## Development

```bash
# Run with debug logging
# Edit config.py: LoggingConfig(level="DEBUG")
python -m server.main
```